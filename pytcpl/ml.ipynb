{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML pipeline for a single assay endpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, fbeta_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load assay endpoint and QSAR-ready chemical structure fingerprints\n",
    "Note: Many chemicals from assays do not have a fingerprint -> small training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Define the root directory\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ROOT_DIR \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m\"\u001b[39m\u001b[39m__file__\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      3\u001b[0m CONFIG_PATH \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ROOT_DIR, \u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mconfig_ml.yaml\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_config\u001b[39m(config_path):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the root directory\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "CONFIG_PATH = os.path.join(ROOT_DIR, 'config', 'config_ml.yaml')\n",
    "\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "        if config[\"ignore_warnings\"]:\n",
    "            import warnings\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    return config  \n",
    "\n",
    " \n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "print(f\"ML pipeline for assay ID: {config['aeid']}\\n\")\n",
    "\n",
    "# Prepare assay df\n",
    "assay_file = f\"{config['aeid']}.csv\"\n",
    "assay_file_path = os.path.join(ROOT_DIR, \"export\", \"out\", assay_file)\n",
    "assay_df = pd.read_csv(assay_file_path)\n",
    "print(f\"Assay dataframe: {assay_df.shape[0]} chemical/hitcall datapoints\")\n",
    "# print(f\"{assay_df['hitc'].value_counts()}\\n\")\n",
    "\n",
    "# Prepare fingerprint df\n",
    "fingerprint_file= \"ToxCast_CSIfps.csv\"\n",
    "fps_file_path = os.path.join(ROOT_DIR, 'input', fingerprint_file)\n",
    "# Skip the first 3 columns (relativeIndex, absoluteIndex, index) and transpose the dataframe\n",
    "fps_df = pd.read_csv(fps_file_path).iloc[:, 3:].T\n",
    "data = fps_df.iloc[1:].values.astype(int)\n",
    "index = fps_df.index[1:]\n",
    "columns = fps_df.iloc[0]\n",
    "fps_df = pd.DataFrame(data=data, index=index, columns=columns).reset_index()\n",
    "fps_df = fps_df.rename(columns={\"index\": \"dtxsid\"})\n",
    "assert fps_df.shape[0] == fps_df['dtxsid'].nunique()\n",
    "print(f\"Fingerprint dataframe ({fingerprint_file}): {fps_df.shape[0]} chemicals, {fps_df.iloc[:, 1:].shape[1]} binary features\")\n",
    "\n",
    "# Get intersection and merge the assay and fingerprint dataframes\n",
    "df = pd.merge(assay_df, fps_df, on=\"dtxsid\").reset_index(drop=True)\n",
    "assert df.shape[0] == df['dtxsid'].nunique()\n",
    "print(f\"\\nMerged dataframe for this ML pipeline: {df.shape[0]} datapoints (chemical fingerprint/hitcall)\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity threshold: (chit >= 0.9 is active)\n",
      "\n",
      "Label Count TOTAL: 7611 datapoints\n",
      " with 7296 inactive, 315 active (4.14%)\n",
      "\n",
      "Label Count TRAIN: 6088 datapoints\n",
      " with 5836 inactive, 252 active (4.14%)\n",
      "\n",
      "Label Count TEST: 1523 datapoints\n",
      " with 1460 inactive, 63 active (4.14%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "# Partition the data into features (X) and labels (y)\n",
    "# Select all columns as fingerprint features, starting from the third column (skipping dtxsid and hitc)\n",
    "X = df.iloc[:, 2:]  \n",
    "# Select the chit column (consensus hit) as the label based on the activity threshold\n",
    "t = config['activity_threshold']\n",
    "print(f\"Activity threshold: (chit >= {t} is active)\\n\")\n",
    "y = (df['chit'] >= t).astype(int)\n",
    "\n",
    "# Split the data into train and test sets before oversampling to avoid data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=config['train_test_split_ratio'],\n",
    "                                                    random_state=config['random_state'],\n",
    "                                                    shuffle=True, # shuffle the data before splitting (default)\n",
    "                                                    stratify=y) # stratify to ensure the same class distribution in the train and test sets\n",
    "def print_label_count(y, title):\n",
    "    counts = y.value_counts().values\n",
    "    print(f\"Label Count {title}: {len(y)} datapoints\\n\"\n",
    "          f\" with {counts[0]} inactive, {counts[1]} active \"\n",
    "          f\"({counts[1]/sum(counts)*100:.2f}%)\\n\")\n",
    "\n",
    "print_label_count(y, \"TOTAL\")\n",
    "print_label_count(y_train, \"TRAIN\")\n",
    "\n",
    "# Perform SMOTE oversampling? within the nested cross-validation loop with hyper-parameter tuning?\n",
    "if config['apply']['smote']:\n",
    "    oversampler = SMOTE(random_state=config['random_state'])\n",
    "    X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "    print_label_count(y_train, \"TRAIN (after oversampling)\")\n",
    "\n",
    "print_label_count(y_test, \"TEST\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build classifier pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "# build for each classifier a pipeline with the steps defined in the config file\n",
    "def build_pipeline(steps):\n",
    "    pipeline_steps = []\n",
    "    for step in steps:\n",
    "        step_name = step['name']  \n",
    "        step_args = step.get('args', {}) # get the hyperparameters for the step, if any\n",
    "        step_instance = globals()[step_name](**step_args)  # dynmically create an instance of the step\n",
    "        pipeline_steps.append((step_name, step_instance))  \n",
    "    return Pipeline(pipeline_steps)\n",
    "\n",
    "\n",
    "def build_param_grid(classifier_steps):\n",
    "    param_grid = {}\n",
    "    for step in classifier_steps:\n",
    "        step_name = step['name']\n",
    "        step_args = step.get('args', {})\n",
    "        param_grid.update({f'{step_name}__{key}': value for key, value in step_args.items() if isinstance(value, list)})\n",
    "    return param_grid\n",
    "\n",
    "\n",
    "def grid_search_cv(classifier, pipeline):\n",
    "    scoring = config['grid_search_cv']['scoring']\n",
    "    # Define the scoring function using F-beta score if specified in the config file\n",
    "    scorer = make_scorer(fbeta_score, beta=config['grid_search_cv']['beta']) if scoring == 'f_beta' else scoring\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, \n",
    "                               param_grid=build_param_grid(classifier['steps']),\n",
    "                               cv=RepeatedStratifiedKFold(n_splits=5, # outer grid: cross-validation, repeated\n",
    "                                                          n_repeats=3, \n",
    "                                                          random_state=config['random_state']), \n",
    "                               scoring=scorer,\n",
    "                               n_jobs=config[\"grid_search_cv\"][\"n_jobs\"],\n",
    "                               verbose=config[\"grid_search_cv\"][\"verbose\"],\n",
    "                               ).fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"{classifier['name']}: GridSearchCV Results:\")\n",
    "    best_params = grid_search.best_params_ if grid_search.best_params_ else \"default\"\n",
    "    print(f\"Best params:\\n{best_params} with mean cross-validated score: {grid_search.best_score_}\\n\")\n",
    "\n",
    "    return grid_search\n",
    "    \n",
    "\n",
    "def predict_and_report(classifier, best_estimator):\n",
    "    print(f\"Predict..\")\n",
    "    y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "    labels = [True, False] \n",
    "    print(f\"Classification Report {classifier['name']}:\")\n",
    "    print(classification_report(y_test, y_pred, labels=labels))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    tn, fp, fn, tp = cm.ravel()   # Extract values from confusion matrix\n",
    "    print(f\"Total: {len(y_test)} datapoints\")\n",
    "    print(f\"Ground truth: {tn + fp} positive, {tp + fn} negative\")\n",
    "    print(f\"Prediction: {tn + fn} positive, {tp + fp} negative\")\n",
    "\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [\"Positive\", \"Negative\"])\n",
    "    cm_display.plot()\n",
    "    plt.title(f\"Confusion Matrix for {classifier['name']}\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline: Grid Search CV + Prediction + Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline for RandomForestClassifier:\n",
      "Pipeline(steps=[('PCA', PCA(n_components=[10])),\n",
      "                ('RandomForestClassifier',\n",
      "                 RandomForestClassifier(n_estimators=[100, 150]))])\n",
      "\n",
      "Fitting 15 folds for each of 2 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "total_time_start = time.time()\n",
    "\n",
    "for classifier in config['classifiers']:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Build the pipeline for the current classifier\n",
    "    pipeline = build_pipeline(classifier['steps'])\n",
    "    print(f\"Pipeline for {classifier['name']}:\\n{pipeline}\\n\")\n",
    "\n",
    "    # Perform grid search on the extracted hyperparameters\n",
    "    grid_search = grid_search_cv(classifier, pipeline)\n",
    "\n",
    "    # Predict on the test set and best estimator\n",
    "    predict_and_report(classifier, grid_search.best_estimator_)\n",
    "\n",
    "    print(f\"Done: {classifier['name']} >> {round(time.time() - start_time, 2)} seconds.\\n{'_' * 75}\\n\\n\")\n",
    "\n",
    "print(f\"Done. Total time >> {round(time.time() - total_time_start, 2)} seconds.\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytcpl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
